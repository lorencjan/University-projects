{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6488,
     "status": "ok",
     "timestamp": 1649488370408,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "107a86a8-bf4d-4ff5-ac8a-c844ef1da303",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "Ex7I8uiDnM75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cNWnrK05fKj"
   },
   "source": [
    "Select run configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1649488370410,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "id": "m5JTgaIa5evd"
   },
   "outputs": [],
   "source": [
    "# 'MF' - baseline ... just standard Matrix Factorization\n",
    "# 'NCF' - collaborative ... using NeuMF (GMF==MF + MLP) architecture according to the paper\n",
    "# 'hybrid' - final model ... duplicated NCF with one branch using pretrained book embeddings \n",
    "architecture = \"hybrid\"\n",
    "\n",
    "# 'colab' vs 'paperspace'\n",
    "environment = \"paperspace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5797946b-272f-4075-bb2d-bfccaa9e7c9e",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "XFZKPwAONJ-d"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 56662,
     "status": "ok",
     "timestamp": 1649488427063,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "f956b871-0d7a-4f14-9626-0ac902b35443",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "ZmhioapnnP-j",
    "outputId": "7139a9fc-b102-4808-f063-fe8037f6d815",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if environment == \"colab\":\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=True)\n",
    "    data_path = \"/content/drive/Shareddrives/KNN-Recommenders/data/\"\n",
    "else:\n",
    "    data_path = \"/notebooks/data/\"\n",
    "    \n",
    "df = pd.read_csv(data_path + \"book_interactions_comics_graphic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "618346a3-4586-4b8d-9b42-733128f4c3b4",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "3z6gL08CMYEH"
   },
   "source": [
    "It's necessary to tranform ids to labels as they can have higher values than their count (index errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1649488427664,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "ef6d8738-556d-42e5-8a95-97ab9697c372",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "8_8GD8QpMYx-",
    "outputId": "073b6a53-dd22-4456-b432-210eda7b5ca8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "df[\"user_label\"] = labelEncoder.fit_transform(df[\"user_id\"])\n",
    "df[\"book_label\"] = labelEncoder.fit_transform(df[\"book_id\"])\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-3ILbbM8P4J"
   },
   "source": [
    "Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1649488427667,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "00bd0708-567f-47ed-93e2-f286e2b3f888",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "yOdP-pIjkHLW",
    "outputId": "3a58b3dd-6411-412b-fcea-338954d32ad3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_users = df[\"user_id\"].unique().shape[0]\n",
    "num_books = df[\"book_id\"].unique().shape[0]\n",
    "df_len = df.shape[0]\n",
    "print(\"Users: {}\".format(num_users))\n",
    "print(\"Books: {}\".format(num_books))\n",
    "print(\"Interactions: {}\".format(df_len))\n",
    "print(\"Density: {}/{} ... {} %\".format(df_len, num_users*num_books, round(100*df_len/(num_users*num_books), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAhGjWgYFeK6"
   },
   "source": [
    "In case of final hybrid model, load also the pretrained book embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1649488427669,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "id": "bzm42w_xFkXz"
   },
   "outputs": [],
   "source": [
    "def csv_embedding_to_dict(filename):\n",
    "    df_embed = pd.read_csv(data_path + filename)\n",
    "    df_embed.set_index(\"book_id\", inplace=True)\n",
    "    return {book_id_to_label[id]:row.values.tolist() for id, row in df_embed.iterrows()}\n",
    "\n",
    "if architecture == \"hybrid\":\n",
    "    book_id_to_label = {row[\"book_id\"]:row[\"book_label\"] for i, row in df.drop_duplicates(\"book_id\").iterrows()}\n",
    "    pretrained_book_embeddings = csv_embedding_to_dict(\"book_embedding_comics_graphic_64.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "98ef50ca-119d-4cfe-a6a5-fa10ec02afbe",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "gilXSI-OrP9J"
   },
   "source": [
    "Use GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1649488427670,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "d3c50abe-5673-4b29-b543-e40e8555d650",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "z4uVpE0iFYXh",
    "outputId": "17e24e5b-bae7-43fc-a718-67bbb79f0d88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "61b04e17-96cb-47ae-b010-e2091378cc49",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "4gwqC5Eun1Zx"
   },
   "source": [
    "Prepare torch dataloaders for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1649488427672,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 7,
     "id": "22ae08c0-f21a-4de6-89c9-1bcf5d7328b0",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "tm1kMCDwzGRV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSet(data.Dataset):\n",
    "    \"\"\" Base dataset for data loaders \"\"\"\n",
    "    def __init__(self, users, books, ratings):\n",
    "        super(DataSet, self).__init__()\n",
    "        self.users = torch.tensor(users, dtype=torch.long, device=device)\n",
    "        self.items = torch.tensor(books, dtype=torch.long, device=device)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float, device=device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2621,
     "status": "ok",
     "timestamp": 1649488430269,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 8,
     "id": "856c7515-760d-460d-9150-c52b77d2edb7",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "KyijIBRBnnBq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256 # proposed in NCF paper + lower doesn't have better results, just trains longer (tried 32, 64, 128)\n",
    "\n",
    "# split to train, validation, test datasets ... 70-20-10\n",
    "if not os.path.exists(data_path + \"train-df.csv\"):\n",
    "    train_df, valid_df, test_df = np.split(df.sample(frac=1), [int(.7 * df_len), int(.9 * df_len)])\n",
    "    # session may be terminated, so to remember\n",
    "    train_df.to_csv(data_path + \"train-df.csv\", index=False)\n",
    "    valid_df.to_csv(data_path + \"valid-df.csv\", index=False)\n",
    "    test_df.to_csv(data_path + \"test-df.csv\", index=False)\n",
    "else:\n",
    "    train_df = pd.read_csv(data_path + \"train-df.csv\")\n",
    "    valid_df = pd.read_csv(data_path + \"valid-df.csv\")\n",
    "    test_df = pd.read_csv(data_path + \"test-df.csv\")\n",
    "\n",
    "# create datasets\n",
    "train_dataset = DataSet(train_df[\"user_label\"].values, train_df[\"book_label\"].values, train_df[\"rating\"].values)\n",
    "valid_dataset = DataSet(valid_df[\"user_label\"].values, valid_df[\"book_label\"].values, valid_df[\"rating\"].values)\n",
    "test_dataset = DataSet(test_df[\"user_label\"].values, test_df[\"book_label\"].values, test_df[\"rating\"].values)\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1dc8ecf1-5db7-42ca-b075-8265e4658216",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "gX3RMUqysVQY"
   },
   "source": [
    "Model - definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path for saving for given architecture\n",
    "model_path = \"{}{}-model\".format(data_path, architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1649488468507,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 9,
     "id": "9d38587e-6275-4840-990d-e69a125128cc",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "jsf77HNMsY-I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspired by https://github.com/guoyang9/NCF/blob/master/model.py\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, user_num, book_num, embedding_dim=32, num_of_layers=3):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # baseline MF embeddings\n",
    "        self.embed_user_GMF = nn.Embedding(user_num, embedding_dim)\n",
    "        self.embed_book_GMF = nn.Embedding(book_num, embedding_dim)\n",
    "        if architecture == \"MF\":\n",
    "            self.create_predict_layer_and_init_weights(embedding_dim)\n",
    "            return\n",
    "        \n",
    "        # added MLP branch of NCF architecture\n",
    "        embed_mul_lambda = lambda x: 2 ** (num_of_layers - x)\n",
    "        self.embed_user_MLP = nn.Embedding(user_num, embedding_dim * embed_mul_lambda(1))\n",
    "        self.embed_book_MLP = nn.Embedding(book_num, embedding_dim * embed_mul_lambda(1))\n",
    "        \n",
    "        MLP_layers = []\n",
    "        dropouts = [0.5] + [0.3] * (num_of_layers - 1)\n",
    "        for i in range(num_of_layers):\n",
    "            dim = embedding_dim * embed_mul_lambda(i)\n",
    "            MLP_layers.append(nn.Dropout(p=dropouts[i]))\n",
    "            MLP_layers.append(nn.Linear(dim, dim // 2))\n",
    "            MLP_layers.append(nn.ReLU())\n",
    "            \n",
    "        self.MLP_layers = nn.Sequential(*MLP_layers)\n",
    "        if architecture == \"NCF\":\n",
    "            self.create_predict_layer_and_init_weights(2 * embedding_dim) # GMF + MLP concatenates -> twice the length\n",
    "            return\n",
    "        \n",
    "        # to NeuMF is concatenated pretrained book embedding of the same size\n",
    "        self.create_predict_layer_and_init_weights(4 * embedding_dim)\n",
    "    \n",
    "    def create_predict_layer_and_init_weights(self, dimension):\n",
    "        \"\"\" Helper method for creating last prediction layer and\n",
    "            initializing weights as different architectures are supported.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.predict_layer = nn.Linear(dimension, 1)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\" Initializes model according to original NCF paper. \"\"\"\n",
    "        \n",
    "        # Xavier for prediction - in paper they used kaiming but we don't have sigmoid activation\n",
    "        nn.init.xavier_uniform_(self.predict_layer.weight)\n",
    "        \n",
    "        # embeddings from normal distribution\n",
    "        standard_deviation = 0.01\n",
    "        nn.init.normal_(self.embed_user_GMF.weight, std=standard_deviation)\n",
    "        nn.init.normal_(self.embed_book_GMF.weight, std=standard_deviation)\n",
    "        if architecture == \"MF\":\n",
    "            return\n",
    "\n",
    "        nn.init.normal_(self.embed_user_MLP.weight, std=standard_deviation)\n",
    "        nn.init.normal_(self.embed_book_MLP.weight, std=standard_deviation)\n",
    "        \n",
    "        # use Xavier for the MLP network\n",
    "        for layer in [x for x in self.MLP_layers if isinstance(x, nn.Linear)]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "    def forward(self, users, books):\n",
    "        \"\"\" Implementation of pytorch nn.Module forward method == computation. \"\"\"\n",
    "        \n",
    "        output_GMF = self.embed_user_GMF(users) * self.embed_book_GMF(books)\n",
    "        if architecture == \"MF\":\n",
    "            return self.predict_layer(output_GMF).view(-1)\n",
    "\n",
    "        output_MLP = self.MLP_layers(torch.cat([self.embed_user_MLP(users), self.embed_book_MLP(books)], -1))\n",
    "        if architecture == \"NCF\":\n",
    "            return self.predict_layer(torch.cat((output_GMF, output_MLP), -1)).view(-1)\n",
    "        \n",
    "        pretrained_embeddings = [pretrained_book_embeddings[label] for label in books.tolist()]\n",
    "        pretrained_embeddings = torch.tensor(pretrained_embeddings, device=device)\n",
    "        return self.predict_layer(torch.cat((pretrained_embeddings, output_GMF, output_MLP), -1)).view(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1649488430857,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "execution_count": 10,
     "id": "1b472ee3-40e6-48fd-9195-e56bd6bbae4c",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "iX_vPjLP56gK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\" Class responsible for training the model. \"\"\"\n",
    "\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader):\n",
    "        self.model = model\n",
    "        self.train_data = train_dataloader\n",
    "        self.valid_data = valid_dataloader\n",
    "        self.batch_iters = {\"Train\": len(train_dataloader), \"Valid\": len(valid_dataloader)}\n",
    "        self.epochs = 1\n",
    "        self.loss_values = {\"Train\": [], \"Valid\": []}\n",
    "        self.best_loss = 1e6\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train(self, epochs=6):\n",
    "        \"\"\" Standard model training. In each batch are updated statistics.\n",
    "            At the end of each epoch the current model is saved and validation run.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()\n",
    "            \n",
    "            # Adam overfits extremely quickly here (almost done after first epoch) -> SGD to slow down\n",
    "            if epoch == 2:\n",
    "                self.optimizer = optim.SGD(model.parameters(), lr=5e-4)\n",
    "                        \n",
    "            loss_sum = 0\n",
    "            for users, books, ratings in self.train_data:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self.model(users, books)\n",
    "                loss = self.criterion(predictions, ratings)\n",
    "                loss_sum += loss.item()\n",
    "                \n",
    "                loss.backward()    \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.eval_epoch(loss_sum, \"Train\", epoch)\n",
    "            self.validate(epoch)\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        \"\"\" Validates the model after each epoch on validation dataset. \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        loss_sum = 0\n",
    "        for users, books, ratings in self.valid_data:\n",
    "            predictions = self.model(users, books)\n",
    "            loss = self.criterion(predictions, ratings)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        self.eval_epoch(loss_sum, \"Valid\", epoch)\n",
    "        \n",
    "    def eval_epoch(self, loss_sum, phase, epoch):\n",
    "        \"\"\" Helper method for finalizing and printing epoch statistics. \"\"\"\n",
    "\n",
    "        data = self.train_data if phase == \"Train\" else self.valid_data\n",
    "        count = len(data.dataset.items)\n",
    "        loss = loss_sum / self.batch_iters[phase]\n",
    "        self.loss_values[phase].append(loss) # update for plot\n",
    "        \n",
    "        # save the best in case we overtrain - quite fast in collaborative filtering\n",
    "        if phase == \"Valid\" and loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.save_model(model_path)\n",
    "        \n",
    "        print_stats = [phase, epoch, self.epochs, loss]\n",
    "        print(\"{}: Epoch: [{}/{}]  Loss: {:.6f} \".format(*print_stats))\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\" Plots loss during training and validation. \"\"\"\n",
    "\n",
    "        plt.plot(self.loss_values[\"Train\"], label = \"Train\")\n",
    "        plt.plot(self.loss_values[\"Valid\"], label = \"Val\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, location=\"model\"):\n",
    "        \"\"\" Saves model to specified location. \"\"\"\n",
    "        \n",
    "        torch.save(self.model.state_dict(), location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_users, num_books).to(device)\n",
    "trainer = ModelTrainer(model, train_dataloader, valid_dataloader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1649488434505,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "id": "1a639c3c-4416-4187-8515-d5f6a472af49",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "x8ovcNMCKv9N"
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "90f5ba23-35f6-4eb3-8913-f8fa4ec2e8f6",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "Mj42BOIIFfpg"
   },
   "source": [
    "Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1649488434506,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "gradient": {
     "editing": false,
     "id": "f506b68d-8c85-4956-a508-2250d655da80",
     "kernelId": "e5dbd247-c199-464c-ac8c-0d673e09664d"
    },
    "id": "i_KixPyT-Dwp"
   },
   "outputs": [],
   "source": [
    "class ModelTester:\n",
    "    \"\"\" Loads a trained model and runs it against test dataset. \"\"\"\n",
    "    \n",
    "    def __init__(self, model, location):\n",
    "        self.model = model.to(device)\n",
    "        self.model.load_state_dict(torch.load(location))\n",
    "        self.model.eval()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.stats = {}\n",
    "        self.predictions = []\n",
    "\n",
    "    def test(self, test_dataset, test_df):\n",
    "        \"\"\" Tests the model against given data \"\"\"\n",
    "\n",
    "        self.stats = {\"loss\": 0, \"hits\": 0}\n",
    "        self.predictions = []\n",
    "        \n",
    "        for user, book, rating in test_dataset:\n",
    "            # make prediction\n",
    "            prediction = self.model(torch.reshape(user, (-1,)), torch.reshape(book, (-1,)))\n",
    "            self.predictions.append(prediction.item())   \n",
    "            \n",
    "            # compute statistics\n",
    "            self.stats[\"loss\"] += self.criterion(prediction.squeeze(), rating).item()\n",
    "            if rating == round(prediction.item()):\n",
    "                self.stats[\"hits\"] += 1\n",
    "                    \n",
    "        # print results\n",
    "        count = len(test_dataset.items)              \n",
    "        print(\"Test dataset metrics: \")\n",
    "        print(\"Loss: {:.6f}\".format(self.stats[\"loss\"] / count  ))\n",
    "        print(\"Hit acc: {:.3f} %\".format(100 * self.stats[\"hits\"] / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1649488434507,
     "user": {
      "displayName": "Jan Lorenc",
      "userId": "14473187453676130053"
     },
     "user_tz": -120
    },
    "id": "I8F-7EA-DaR6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tester = ModelTester(Model(num_users, num_books), model_path)\n",
    "tester.test(test_dataset, test_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NCF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
