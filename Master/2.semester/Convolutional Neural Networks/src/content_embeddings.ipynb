{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KwQZRSS7EvR"
      },
      "source": [
        "**Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_53Ineip7H2t"
      },
      "outputs": [],
      "source": [
        "# Uses only the first 20000 books\n",
        "use_shorter_dataset = False\n",
        "\n",
        "# Number of epochs to train the autoencoder for - around 30 seems to be ideal for poetry\n",
        "# Some experimentation might be needed with other datasets, see a plot of the training below\n",
        "epochs = 75\n",
        "\n",
        "# Specify the genre to use - this is used to construct the filenames\n",
        "genre = \"kaggle\"\n",
        "\n",
        "# Size of the embedding outputted by the encoder (string part of the embedding)\n",
        "# The int part is concatenated to this embedding, so the final size will be\n",
        "# string_embedding_size + 5\n",
        "string_embedding_size = 251\n",
        "\n",
        "embedding_field_name = \"embedding\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa-hBWD37H3"
      },
      "source": [
        "**String embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYQD9ZVbwBoR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnNq1qUuxzcf"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PEIKytyFx31b"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive_data_path = \"/content/drive/Shareddrives/KNN-Recommenders/data/\"\n",
        "data_string = pd.read_csv(drive_data_path + \"book_metadata_string_\" + genre + \".csv\")\n",
        "if use_shorter_dataset:\n",
        "  data_string = data_string[0:20000]\n",
        "data_string.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLiTH6Yt4UDq"
      },
      "source": [
        "Create the tfidf vectorizer and extract data for 10000 most used words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yyvaZ7zDzdrn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), min_df=0.0001, stop_words='english', max_features=10000)\n",
        "tfidf_matrix = tfidf.fit_transform(data_string[embedding_field_name])\n",
        "tfidf_data = pd.DataFrame(tfidf_matrix.toarray(), index=data_string.index.tolist())\n",
        "\n",
        "print(tfidf_data.shape)\n",
        "tfidf_data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl4-ao814hsn"
      },
      "source": [
        "Autoencoder. Source: https://github.com/alineberry/my-movie-recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ym4V6FuT4fkV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import multiprocessing\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "def get_cv_idxs(n, cv_idx=0, val_pct=0.2, seed=42):\n",
        "    \"\"\" Get a list of index values for Validation set from a dataset\n",
        "    Arguments:\n",
        "        n : int, Total number of elements in the data set.\n",
        "        cv_idx : int, starting index [idx_start = cv_idx*int(val_pct*n)]\n",
        "        val_pct : (int, float), validation set percentage\n",
        "        seed : seed value for RandomState\n",
        "    Returns:\n",
        "        list of indexes\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    n_val = int(val_pct * n)\n",
        "    idx_start = cv_idx * n_val\n",
        "    idxs = np.random.permutation(n)\n",
        "    return idxs[idx_start:idx_start + n_val]\n",
        "\n",
        "\n",
        "def split_by_idx(idxs, *a):\n",
        "    \"\"\"\n",
        "    Split each array passed as *a, to a pair of arrays like this (elements selected by idxs,  the remaining elements)\n",
        "    This can be used to split multiple arrays containing training data to validation and training set.\n",
        "    :param idxs [int]: list of indexes selected\n",
        "    :param a list: list of np.array, each array should have same amount of elements in the first dimension\n",
        "    :return: list of tuples, each containing a split of corresponding array from *a.\n",
        "            First element of each tuple is an array composed from elements selected by idxs,\n",
        "            second element is an array of remaining elements.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(len(a[0]), dtype=bool)\n",
        "    mask[np.array(idxs)] = True\n",
        "    return [(o[mask], o[~mask]) for o in a]\n",
        "\n",
        "\n",
        "class AutoEncoder(object):\n",
        "\n",
        "    def __init__(self, data, validation_perc=0.2, lr=0.001,\n",
        "                 intermediate_size=1000, encoded_size=100):\n",
        "\n",
        "        # create training dataloader and validation tensor\n",
        "        self.data = data\n",
        "        self.val_idxs = get_cv_idxs(n=data.shape[0], val_pct=validation_perc)\n",
        "        [(self.val, self.train)] = split_by_idx(self.val_idxs, data)\n",
        "        self.dataset = AETrainingData(self.train)\n",
        "        self.dataloader = DataLoader(self.dataset, batch_size=64, shuffle=True,\n",
        "                                     num_workers=multiprocessing.cpu_count())\n",
        "        self.val = torch.from_numpy(self.val.values).\\\n",
        "            type(torch.FloatTensor).cuda()\n",
        "\n",
        "        # instantiate the encoder and decoder nets\n",
        "        size = data.shape[1]\n",
        "        self.encoder = Encoder(size, intermediate_size, encoded_size).cuda()\n",
        "        self.decoder = Decoder(size, intermediate_size, encoded_size).cuda()\n",
        "\n",
        "        # instantiate the optimizers\n",
        "        self.encoder_optimizer = optim.Adam(\n",
        "            self.encoder.parameters(), lr=lr, weight_decay=1e-8)\n",
        "        self.decoder_optimizer = optim.Adam(\n",
        "            self.decoder.parameters(), lr=lr, weight_decay=1e-8)\n",
        "\n",
        "        # instantiate the loss criterion\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train_step(self, input_tensor, target_tensor):\n",
        "        # clear the gradients in the optimizers\n",
        "        self.encoder_optimizer.zero_grad()\n",
        "        self.decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through\n",
        "        encoded_representation = self.encoder(input_tensor)\n",
        "        reconstruction = self.decoder(encoded_representation)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = self.criterion(reconstruction, target_tensor)\n",
        "\n",
        "        # Compute the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizers to update the model weights\n",
        "        self.encoder_optimizer.step()\n",
        "        self.decoder_optimizer.step()\n",
        "\n",
        "        # Return the loss value to track training progress\n",
        "        return loss.item()\n",
        "    \n",
        "    def reset(self, train=True):\n",
        "        # due to dropout the network behaves differently in training and\n",
        "        # evaluation modes\n",
        "        if train: self.encoder.train(); self.decoder.train()\n",
        "        else: self.encoder.eval(); self.decoder.eval()\n",
        "\n",
        "    def get_val_loss(self, input_tensor, target_tensor):\n",
        "        self.reset(train=False)\n",
        "        encoded = self.encoder(input_tensor)\n",
        "        decoded = self.decoder(encoded)\n",
        "        loss = self.criterion(decoded, target_tensor)\n",
        "        return loss.item()\n",
        "\n",
        "    def train_loop(self, epochs, print_every_n_batches=100):\n",
        "\n",
        "        # Cycle through epochs\n",
        "        for epoch in range(epochs):\n",
        "            print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "            # Cycle through batches\n",
        "            for i, batch in enumerate(self.dataloader):\n",
        "                \n",
        "                self.reset(train=True)\n",
        "\n",
        "                input_tensor = batch['input'].cuda()\n",
        "                target_tensor = batch['target'].cuda()\n",
        "\n",
        "                loss = self.train_step(input_tensor, target_tensor)\n",
        "\n",
        "                if i % print_every_n_batches == 0 and i != 0:\n",
        "                    val_loss = self.get_val_loss(self.val, self.val)\n",
        "                    print(f'train loss: {round(loss, 8)} | ' +\n",
        "                          f'validation loss: {round(val_loss, 8)})')\n",
        "                    self.train_losses.append(loss)\n",
        "                    self.val_losses.append(val_loss)\n",
        "\n",
        "    def get_encoded_representations(self):\n",
        "        to_encode = torch.from_numpy(self.data.values).type(\n",
        "            torch.FloatTensor).cuda()\n",
        "        self.reset(train=False)\n",
        "        encodings = self.encoder(to_encode).cpu().data.numpy()\n",
        "        return encodings\n",
        "\n",
        "\n",
        "class AETrainingData(Dataset):\n",
        "    \"\"\"\n",
        "    Format the training dataset to be input into the auto encoder.\n",
        "    Takes in dataframe and converts it to a PyTorch Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_train):\n",
        "        self.x = x_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a example from the data set as a pytorch tensor.\n",
        "        \"\"\"\n",
        "        # Get example/target pair at idx as numpy arrays\n",
        "        x, y = self.x.iloc[idx].values, self.x.iloc[idx].values\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        x = torch.from_numpy(x).type(torch.FloatTensor)\n",
        "        y = torch.from_numpy(y).type(torch.FloatTensor)\n",
        "\n",
        "        # Return pair\n",
        "        return {'input': x, 'target': y}\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, intermediate_size, encoding_size):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, intermediate_size),\n",
        "            nn.BatchNorm1d(intermediate_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(intermediate_size, encoding_size),\n",
        "            nn.BatchNorm1d(encoding_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, intermediate_size, encoding_size):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_size, intermediate_size),\n",
        "            nn.BatchNorm1d(intermediate_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(intermediate_size, output_size),\n",
        "            nn.BatchNorm1d(output_size),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y6I-EU142_f"
      },
      "source": [
        "Train autoencoder and plot a graph. Change \"encoded_size\" to specify a different size of the final encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C1op0PAZ41LI"
      },
      "outputs": [],
      "source": [
        "ae = AutoEncoder(tfidf_data, validation_perc=0.1, lr=1e-3, intermediate_size=5000, encoded_size=string_embedding_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q665pD736NIg"
      },
      "outputs": [],
      "source": [
        "ae.train_loop(epochs=epochs)\n",
        "losses = pd.DataFrame(data=list(zip(ae.train_losses, ae.val_losses)), columns=['train_loss', 'validation_loss'])\n",
        "losses['epoch'] = (losses.index + 1) / 3\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(losses['epoch'], losses['train_loss'])\n",
        "ax.plot(losses['epoch'], losses['validation_loss'])\n",
        "ax.set_ylabel('MSE loss')\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_title('autoencoder loss over time')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_bF2n8-kC2WT"
      },
      "outputs": [],
      "source": [
        "encoded = ae.get_encoded_representations()\n",
        "print(encoded.shape)\n",
        "encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BudfLpUgEJud"
      },
      "source": [
        "Code for checking similarity between books. Good for checking the embeddings give sense. Source: https://github.com/alineberry/my-movie-recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B02zys0hELYJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import pandas as pd\n",
        "\n",
        "class SimilarityPredictions(object):\n",
        "    '''This class calculates a similarity matrix from latent embeddings.\n",
        "    There is a method to save this similarity model locally, and a method for\n",
        "    predicting similar items from the matrix.\n",
        "    Input: embeddings - a pandas dataframe of items and latent dimensions.\n",
        "            similarity_metric = str definining the similarity metrics to use'''\n",
        "\n",
        "    def __init__(self, embeddings, similarity_metric='cosine'):\n",
        "        assert similarity_metric in ['cosine', 'euclidean'], \"unsupported similarity metric.\"\n",
        "        self.embeddings = embeddings\n",
        "        self.ids = embeddings.index.tolist()\n",
        "        self.similarity_metric = similarity_metric\n",
        "        if similarity_metric == 'cosine':\n",
        "            self.similarity_matrix = self.calculate_cosine_similarity_matrix()\n",
        "        if similarity_metric == 'euclidean':\n",
        "            self.similarity_matrix = self.calculate_euclidean_distances_matrix()\n",
        "\n",
        "    def calculate_cosine_similarity_matrix(self):\n",
        "        '''Calculates a cosine similarity matrix from the embeddings'''\n",
        "        similarity_matrix = pd.DataFrame(cosine_similarity(\n",
        "            X=self.embeddings),\n",
        "            index=self.ids)\n",
        "        similarity_matrix.columns = self.ids\n",
        "        return similarity_matrix\n",
        "\n",
        "    def calculate_euclidean_distances_matrix(self):\n",
        "        '''Calculates a cosine similarity matrix from the embeddings'''\n",
        "        similarity_matrix= pd.DataFrame(euclidean_distances(\n",
        "            X=self.embeddings),\n",
        "            index=self.ids)\n",
        "        similarity_matrix.columns = self.ids\n",
        "        return similarity_matrix\n",
        "\n",
        "    def predict_similar_items(self, seed_item, n):\n",
        "        '''Use the similarity_matrix to return n most similar items.'''\n",
        "        similar_items = pd.DataFrame(self.similarity_matrix.loc[seed_item])\n",
        "        similar_items.columns = [\"similarity_score\"]\n",
        "        if self.similarity_metric == 'cosine':\n",
        "            similar_items = similar_items.sort_values('similarity_score', ascending=False)\n",
        "        if self.similarity_metric == 'euclidean':\n",
        "            similar_items = similar_items.sort_values('similarity_score', ascending=True)\n",
        "        similar_items = similar_items.head(n)\n",
        "        similar_items.reset_index(inplace=True)\n",
        "        similar_items = similar_items.rename(index=str, columns={\"index\": \"item_id\"})\n",
        "        return similar_items.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo2mNy_g8T5F"
      },
      "source": [
        "This code can be used to check similar books\n",
        "It consumes RAM, so it's commented for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T9Ckq885EX0d"
      },
      "outputs": [],
      "source": [
        "#content_embeddings = pd.DataFrame(encoded)\n",
        "#sim_model_cont = SimilarityPredictions(content_embeddings, similarity_metric=\"cosine\")\n",
        "#cont_output = sim_model_cont.predict_similar_items(seed_item=4141, n=26744)\n",
        "#similar_movies = pd.DataFrame(cont_output)\n",
        "#similar_movies.set_index('item_id', inplace=True)\n",
        "#sim_df_cont = similar_movies\n",
        "#sim_df_cont.sort_values('similarity_score', ascending=False, inplace=True)\n",
        "#sim_df_cont = sim_df_cont.rename(index=str, columns={\"similarity_score\": \"content_similarity_score\"})\n",
        "#sim_df_cont.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jJLEmi-iV54c"
      },
      "outputs": [],
      "source": [
        "#print(data_string[\"aggregated_string\"][16146])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym6h4MvB8o63"
      },
      "source": [
        "Drop the text data from the data_string data frame and create a new dataframe containing the generated string embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zBsUZzeWXqXZ"
      },
      "outputs": [],
      "source": [
        "data_string.drop([embedding_field_name], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yq_jU0QrYulL"
      },
      "outputs": [],
      "source": [
        "data_string_new = pd.concat([data_string, pd.DataFrame(encoded)], axis=1)\n",
        "data_string_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbHjcUhPUvYP"
      },
      "source": [
        "**Numbers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Z9ROYN9IEw"
      },
      "source": [
        "Read the file containing numeric book data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MtmmO15yUy01"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_int = pd.read_csv(drive_data_path + \"book_metadata_int_\" + genre + \".csv\")\n",
        "if use_shorter_dataset:\n",
        "  data_int = data_int[0:20000]\n",
        "num_books = data_int[\"book_id\"].unique().shape[0]\n",
        "data_int.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6aHNxFB9aKe"
      },
      "source": [
        "Rename the columns so it can be merged with string embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yZf3Uy_fcW8K"
      },
      "outputs": [],
      "source": [
        "hight, width = data_string_new.shape\n",
        "col_names = {\"ratings_count\": width - 1, \"num_pages\": width, \"average_rating\": width + 1, \"text_reviews_count\": width + 2}\n",
        "data_int.rename(columns = col_names, inplace = True)\n",
        "data_int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJm9K6MC9i5P"
      },
      "source": [
        "Normalize the int data, so that the values are inside <string_embedding_min; string_embedding_max>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NQzFu3f5enYQ"
      },
      "outputs": [],
      "source": [
        "def normalize(value, min, max, new_max):\n",
        "    return ((value - min) / (max - min)) * new_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j9dbE0veVIu3"
      },
      "outputs": [],
      "source": [
        "maxes = list()\n",
        "for i in range(0, string_embedding_size):\n",
        "  maxes.append(data_string_new[i].max())\n",
        "data_string_max = max(maxes)\n",
        "\n",
        "for i in col_names.values():\n",
        "  data_int[i] = normalize(data_int[i], data_int[i].min(), data_int[i].max(), data_string_max)\n",
        "data_int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmGtiiAg90Nm"
      },
      "source": [
        "Merge the created int embedding into string embedding data frame created previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "01vtm2dOgyk2"
      },
      "outputs": [],
      "source": [
        "merged_df = data_string_new.merge(data_int, left_on=\"book_id\", right_on=\"book_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkgWg1feGD1_"
      },
      "source": [
        "Add normalized ids at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c_z0a9XPGKxg"
      },
      "outputs": [],
      "source": [
        "merged_df[width + 3] = normalize(merged_df[\"book_id\"], merged_df[\"book_id\"].min(), merged_df[\"book_id\"].max(), data_string_max)\n",
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYQ2VS84-Vhw"
      },
      "source": [
        "Save the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x2ufmaTM-XYv"
      },
      "outputs": [],
      "source": [
        "merged_df.to_csv(drive_data_path + \"book_embedding_\" + genre + \"_\" + str(string_embedding_size + 5) + \".csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "content_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}