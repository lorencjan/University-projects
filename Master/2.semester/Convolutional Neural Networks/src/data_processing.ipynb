{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Inserts all necessary libraries"
      ],
      "metadata": {
        "id": "6PMkUaDgFgRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw3Ja82p-MDD"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "from os import path\n",
        "import pickle\n",
        "import csv\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount google drive"
      ],
      "metadata": {
        "id": "-g4QCQebqtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "drive_path = \"/content/drive/Shareddrives/KNN-Recommenders/data/\""
      ],
      "metadata": {
        "id": "uC02m5GdocVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d75169dc-919f-4610-c9a3-383c5f14048d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set dataset variant to download"
      ],
      "metadata": {
        "id": "bJ90YVHJmc9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variant = 0 => all genres | 1 => Poetry | 2 => Comics\n",
        "variant = 2\n",
        "\n",
        "# dataset cleaning\n",
        "# all ... avg 118 interactions per user -> at least 10 for each user\n",
        "# poetry ... avg 4 interactions per user -> at least 2 for each user\n",
        "# comics ... avg 30 interactions per user  -> at least 5 for each user\n",
        "min_interactions = 10 if variant == 0 else 2 if variant == 1 else 5\n",
        "\n",
        "url_book_metadata = {0:\"https://drive.google.com/uc?id=1LXpK1UfqtP89H1tYy0pBGHjYk8IhigUK&confirm=t\",\n",
        "                     1:\"https://drive.google.com/uc?id=1H6xUV48D5sa2uSF_BusW-IBJ7PCQZTS1&confirm=t\",\n",
        "                     2:\"https://drive.google.com/uc?id=1ICk5x0HXvXDp5Zt54CKPh5qz1HyUIn9m&confirm=t\"}\n",
        "\n",
        "filename_metadata = {0:\"goodreads_books.json.gz\",\n",
        "                     1:\"goodreads_books_poetry.json.gz\",\n",
        "                     2:\"goodreads_books_comics_graphic.json.gz\"}\n",
        "\n",
        "csv_metadata_string = {0:\"book_metadata_string_all.csv\",\n",
        "                       1:\"book_metadata_string_poetry.csv\",\n",
        "                       2:\"book_metadata_string_comics_graphic.csv\"}\n",
        "\n",
        "csv_metadata_int = {0:\"book_metadata_int_all.csv\",\n",
        "                    1:\"book_metadata_int_poetry.csv\",\n",
        "                    2:\"book_metadata_int_comics_graphic.csv\"}\n",
        "\n",
        "url_user_interaction = {0:\"https://drive.google.com/u/0/uc?id=1zmylV7XW2dfQVCLeg1LbllfQtHD2KUon&export=download&confirm=t\",\n",
        "                        1:\"https://drive.google.com/uc?id=17G5_MeSWuhYnD4fGJMvKRSOlBqCCimxJ&confirm=t\",\n",
        "                        2:\"https://drive.google.com/uc?id=1CCj-cQw_mJLMdvF_YYfQ7ibKA-dC_GA2&confirm=t\"}\n",
        "\n",
        "filename_user_interaction = {0:\"goodreads_interactions.csv\",\n",
        "                             1:\"goodreads_interactions_poetry.json.gz\",\n",
        "                             2:\"goodreads_interactions_comics_graphic.json.gz\"}\n",
        "\n",
        "csv_interaction = {0:\"book_interactions_all.csv\",\n",
        "                   1:\"book_interactions_poetry.csv\",\n",
        "                   2:\"book_interactions_comics_graphic.csv\"}"
      ],
      "metadata": {
        "id": "sokP52BoiTU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download JSON with author names and IDs"
      ],
      "metadata": {
        "id": "xOXidK1_EWeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_authors = \"https://drive.google.com/uc?id=19cdwyXwfXx_HDIgxXaHzH0mrx8nMyLvC\"\n",
        "filename_authors = \"authors.json.gz\"\n",
        "\n",
        "if not path.exists(drive_path + filename_authors):\n",
        "    with open(drive_path + filename_authors, \"wb\") as f:\n",
        "        r = requests.get(url_authors)\n",
        "        f.write(r.content)"
      ],
      "metadata": {
        "id": "nToaASPbXPHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and save dict {author_id : author_name}"
      ],
      "metadata": {
        "id": "P0UwW5SgErHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "author_name_id = {}\n",
        "if not path.exists(drive_path + \"author_name_id.pkl\"):\n",
        "    with gzip.GzipFile(drive_path + filename_authors, \"rb\") as fin:\n",
        "        for line in fin:\n",
        "            line_dics = json.loads(line.decode(\"utf-8\"))\n",
        "            author_name_id[line_dics[\"author_id\"]] = line_dics[\"name\"]\n",
        "    \n",
        "    with open(drive_path + \"author_name_id.pkl\", \"wb\") as f:\n",
        "        pickle.dump(author_name_id, f)\n",
        "else:\n",
        "    with open(drive_path + \"author_name_id.pkl\", \"rb\") as f:\n",
        "        author_name_id = pickle.load(f)"
      ],
      "metadata": {
        "id": "1XO1GrYYb1O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Book and user number to id mapping"
      ],
      "metadata": {
        "id": "91PeUIdgadVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_booknumber_to_bookid = \"https://drive.google.com/uc?id=1CHTAaNwyzvbi1TR08MJrJ03BxA266Yxr\"\n",
        "filename_booknumber_to_bookid = \"booknumber_to_bookid.csv\"\n",
        "if not path.exists(drive_path + filename_booknumber_to_bookid):\n",
        "    with open(drive_path + filename_booknumber_to_bookid, \"wb\") as f:\n",
        "        r = requests.get(url_booknumber_to_bookid)\n",
        "        f.write(r.content)"
      ],
      "metadata": {
        "id": "PQmaMubDac-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_user_id_to_number = \"https://drive.google.com/uc?id=15ax-h0Oi_Oyee8gY_aAQN6odoijmiz6Q\"\n",
        "filename_user_id_to_number = \"user_id_map\"\n",
        "if not path.exists(drive_path + filename_user_id_to_number):\n",
        "    with open(drive_path + filename_user_id_to_number, \"wb\") as f:\n",
        "        r = requests.get(url_user_id_to_number)\n",
        "        f.write(r.content)"
      ],
      "metadata": {
        "id": "MxEuxW2hrtT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and process data of books genres"
      ],
      "metadata": {
        "id": "YN9a9SCgFQSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_genres = \"https://drive.google.com/uc?id=1ah0_KpUterVi-AHxJ03iKD6O0NfbK0md\"\n",
        "filename_genres = \"gooreads_book_genres_initial.json.gz\"\n",
        "\n",
        "if not path.exists(drive_path + filename_genres):\n",
        "    with open(drive_path + filename_genres, \"wb\") as f:\n",
        "        r = requests.get(url_genres)\n",
        "        f.write(r.content)\n",
        "\n",
        "books_genres = {}\n",
        "if not path.exists(drive_path + \"books_genres.pkl\"):\n",
        "    with gzip.GzipFile(drive_path + filename_genres, \"rb\") as f:\n",
        "        for line in f:\n",
        "            line_dics = json.loads(line.decode(\"utf-8\"))\n",
        "            genres_str = \"\"            \n",
        "            # list -> string\n",
        "            for g in list(line_dics[\"genres\"].keys()):\n",
        "                genres_str = genres_str + \" \" + g\n",
        "                books_genres[line_dics[\"book_id\"]] = genres_str\n",
        "    \n",
        "    with open(drive_path + \"books_genres.pkl\", \"wb\") as f:\n",
        "        pickle.dump(books_genres, f)\n",
        "\n",
        "else:\n",
        "    with open(drive_path + \"books_genres.pkl\", \"rb\") as f:\n",
        "        books_genres = pickle.load(f)"
      ],
      "metadata": {
        "id": "hAYiaru63qQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and process interactions. Drop interactions with rating 0. Remove books with few interactions."
      ],
      "metadata": {
        "id": "WBb0dpT-2uUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not path.exists(drive_path + filename_user_interaction[variant]):\n",
        "    with open(drive_path + filename_user_interaction[variant], \"wb\") as f:\n",
        "        r = requests.get(url_user_interaction[variant])\n",
        "        f.write(r.content)\n",
        "\n",
        "if variant:\n",
        "    book_interactions = []\n",
        "    user_id_map = pd.read_csv(drive_path + \"user_id_map\", names=[\"user_id_csv\", \"user_id\"], index_col = 1, low_memory=False)\n",
        "    with gzip.GzipFile(drive_path + filename_user_interaction[variant], \"rb\") as f:\n",
        "        for line in f:\n",
        "            l = json.loads(line.decode(\"utf-8\"))\n",
        "            if l[\"rating\"] != 0: # we want only explicit interactions -> drop those with rating 0\n",
        "                book_interactions.append([user_id_map.loc[l[\"user_id\"]].user_id_csv, l[\"book_id\"], l[\"rating\"]])\n",
        "\n",
        "    df_interactions = pd.DataFrame(book_interactions, columns=[\"user_id\", \"book_id\", \"rating\"])\n",
        "else:\n",
        "    df_interactions = pd.read_csv(drive_path + filename_user_interaction[variant], usecols=[\"user_id\", \"book_id\", \"rating\"], low_memory=True)\n",
        "    df_interactions = df_interactions[df_interactions.rating != 0] # we want only explicit interactions -> drop those with rating 0\n",
        "\n",
        "# remove interactions where books/users don't have enough\n",
        "old_count = 0\n",
        "while df_interactions.shape[0] != old_count:\n",
        "    old_count = df_interactions.shape[0]\n",
        "    grouped_books = df_interactions.groupby(by=\"book_id\").agg({\"user_id\":\"count\"}).rename(columns={\"user_id\":\"review_count\"})\n",
        "    wanted_books = grouped_books[grouped_books.review_count >= min_interactions]\n",
        "    grouped_users = df_interactions.groupby(by=\"user_id\").agg({\"book_id\":\"count\"}).rename(columns={\"book_id\":\"review_count\"})\n",
        "    wanted_users = grouped_users[grouped_users.review_count >= min_interactions]\n",
        "    df_interactions = df_interactions[df_interactions[\"book_id\"].isin(wanted_books.index) & df_interactions[\"user_id\"].isin(wanted_users.index)]\n",
        "\n",
        "df_interactions.sort_values(by=\"user_id\") \n",
        "\n",
        "df_interactions.to_csv(drive_path + csv_interaction[variant], header=[\"user_id\", \"book_id\", \"rating\"], index=False)"
      ],
      "metadata": {
        "id": "08PYKhdtETrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and process books metadata. We only use some columns. \n"
      ],
      "metadata": {
        "id": "CHNI-vEg1tal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not path.exists(drive_path + filename_metadata[variant]):\n",
        "    with open(drive_path + filename_metadata[variant], \"wb\") as f:\n",
        "        r = requests.get(url_book_metadata[variant])\n",
        "        f.write(r.content)\n",
        "\n",
        "book_metadata_string = []\n",
        "book_metadata_int = []\n",
        "with gzip.GzipFile(drive_path + filename_metadata[variant], \"rb\") as f:   \n",
        "    book_ids = df_interactions[\"book_id\"].unique()\n",
        "    for line in f:\n",
        "        authors = series = \"\"\n",
        "        l = json.loads(line.decode(\"utf-8\"))\n",
        "        book_id = l[\"book_id\"]\n",
        "        #if book doesnt have interaction, continue\n",
        "        if book_id not in book_ids:\n",
        "            continue\n",
        "\n",
        "        #Get all authors with empty role\n",
        "        for author in l[\"authors\"]:\n",
        "            if author[\"role\"] == '':\n",
        "                authors = authors + \" \" + str(author_name_id[author[\"author_id\"]])\n",
        "\n",
        "        #concat all series \n",
        "        for serie in l[\"series\"]:\n",
        "            series = series + \" \" + str(serie)\n",
        "        \n",
        "\n",
        "        book_metadata_string.append([str(book_id),\n",
        "                                     l[\"title\"] + \" \" +\n",
        "                                     l[\"publisher\"] + \" \" + \n",
        "                                     l[\"description\"] + \" \" +\n",
        "                                     authors + \" \" +\n",
        "                                     series + \" \" +\n",
        "                                     books_genres[book_id]])\n",
        "\n",
        "        num_pages = l[\"num_pages\"] if l[\"num_pages\"].isnumeric() else -1\n",
        "        text_reviews_count = l[\"text_reviews_count\"] if l[\"text_reviews_count\"].isnumeric() else -1\n",
        "        ratings_count = l[\"ratings_count\"] if l[\"ratings_count\"].isnumeric() else -1\n",
        "        average_rating = l[\"average_rating\"];\n",
        "        try:\n",
        "            float(average_rating)\n",
        "        except:\n",
        "            average_rating = -1.0\n",
        "\n",
        "        book_metadata_int.append([int(book_id),\n",
        "                                  int(ratings_count),\n",
        "                                  int(num_pages),\n",
        "                                  float(average_rating),\n",
        "                                  int(text_reviews_count)])\n",
        "\n",
        "with open(drive_path + csv_metadata_string[variant], \"w\") as f:\n",
        "    writer = csv.writer(f, delimiter=\",\")\n",
        "    writer.writerow([\"book_id\", \"aggregated_string\"])\n",
        "    writer.writerows(book_metadata_string)\n",
        "\n",
        "with open(drive_path + csv_metadata_int[variant], \"w\") as f:\n",
        "    writer = csv.writer(f, delimiter=\",\")\n",
        "    writer.writerow([\"book_id\", \"ratings_count\", \"num_pages\", \"average_rating\", \"text_reviews_count\"])\n",
        "    writer.writerows(book_metadata_int)"
      ],
      "metadata": {
        "id": "SOYbNhJmnIAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User-book interactions"
      ],
      "metadata": {
        "id": "EOmN4FfA21RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(drive_path + csv_interaction[variant])"
      ],
      "metadata": {
        "id": "g5ULMxnpdHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Int vectors"
      ],
      "metadata": {
        "id": "hzxos0ZR2zOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(drive_path + csv_metadata_int[variant])"
      ],
      "metadata": {
        "id": "SP9tMBUiTrt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "String vectors"
      ],
      "metadata": {
        "id": "XQnn_6O023dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(drive_path + csv_metadata_string[variant])"
      ],
      "metadata": {
        "id": "Z7RuiJievi3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Kaggle 10k dataset for testing purposes"
      ],
      "metadata": {
        "id": "299YuBAoYac3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! chmod 600 /content/drive/Shareddrives/KNN-Recommenders/kaggle.json\n",
        "! cp /content/drive/Shareddrives/KNN-Recommenders/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "RNpRZ--kYbh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goodbooks_filename = drive_path + \"goodbooks-10k.zip\"\n",
        "if not path.exists(goodbooks_filename):\n",
        "    ! kaggle datasets download -d zygmunt/goodbooks-10k -p \"/content/drive/Shareddrives/KNN-Recommenders/data/\""
      ],
      "metadata": {
        "id": "9b8Z3pDKYmmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zf = zipfile.ZipFile(goodbooks_filename)\n",
        "df_books = pd.read_csv(zf.open(\"books.csv\"), usecols=[\"id\", \"book_id\"])\n",
        "df_interactions = pd.read_csv(zf.open(\"ratings.csv\"))\n",
        "# it's obsolete dataset - has duplicates (updated doesn't have so many references to compare + is much bigger)\n",
        "x = df_interactions[[\"book_id\", \"user_id\"]].drop_duplicates().index\n",
        "df_interactions= df_interactions.loc[x, :]\n",
        "\n",
        "# kaggle ratings hav bookids 1-10000 - not real book ids - change\n",
        "id_to_book_id = {row[\"id\"]:row[\"book_id\"] for _, row in df_books.iterrows()}\n",
        "df_interactions[\"book_id\"] = [id_to_book_id[id] for id in df_interactions[\"book_id\"]]\n",
        "\n",
        "# filter existing books by kaggle books - presumes existing all books metadata files\n",
        "df_books = df_books.drop(columns=\"id\")\n",
        "metadata_int_df = pd.read_csv(drive_path + csv_metadata_int[0])\n",
        "metadata_int_df = df_books.merge(metadata_int_df, on=['book_id'])\n",
        "metadata_string_df = pd.read_csv(drive_path + csv_metadata_string[0])\n",
        "metadata_string_df = df_books.merge(metadata_string_df, on=['book_id'])\n",
        "\n",
        "# some books might have been filtered out\n",
        "df_interactions = df_interactions[df_interactions[\"book_id\"].isin(metadata_int_df[\"book_id\"])]\n",
        "\n",
        "# save\n",
        "df_interactions.to_csv(drive_path + \"book_interactions_kaggle.csv\", index=False)\n",
        "metadata_int_df.to_csv(drive_path + \"book_metadata_int_kaggle.csv\", index=False)\n",
        "metadata_string_df.to_csv(drive_path + \"book_metadata_string_kaggle.csv\", index=False)"
      ],
      "metadata": {
        "id": "JwBP9khcYqEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}